{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (5279, 4)\n",
      "Test shape: (2924, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(\"../input/innoplexusav/train.csv\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "test = pd.read_csv(\"../input/innoplexusav/test.csv\")\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why youâ€™d want to ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash    ...    sentiment\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0    ...            2\n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4    ...            2\n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76    ...            2\n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6    ...            2\n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249    ...            1\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Vocabulary Size: 41015\n",
      "Test Set Vocabulary Size: 31570\n",
      "Number of Words that occur in both: 24625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer()\n",
    "cv1.fit(train[\"text\"])\n",
    "\n",
    "cv2 = CountVectorizer()\n",
    "cv2.fit(test[\"text\"])\n",
    "\n",
    "print(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\n",
    "print(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\n",
    "print(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['drug']\n",
    "\n",
    "def encoder(df):\n",
    "    for col in columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(df[col])\n",
    "        df[col] = label_encoder.transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encoder(train)\n",
    "test = encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "    df[\"has_upper\"] = df[\"text\"].apply(lambda x: x.lower() != x)\n",
    "    df[\"sentence_end\"] = df[\"text\"].apply(lambda x: x.endswith(\".\"))\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "dense_features = [\"drug\"]\n",
    "\n",
    "#train.groupby(\"Sentiment\")[dense_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that don't exist in GLOVE: 13381\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "all_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "print(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 60\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(np.append(train[\"text\"].values, test[\"text\"].values))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    sent = textblob.TextBlob(word).sentiment\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n",
    "    else:\n",
    "        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n",
    "        \n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(train[\"text\"]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"text\"]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM + 2,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    dropout = SpatialDropout1D(0.2)\n",
    "    mask_layer = Masking()\n",
    "    lstm_layer = LSTM(50)\n",
    "    \n",
    "    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    dense_input = Input(shape=(len(dense_features),))\n",
    "    \n",
    "    dense_vector = BatchNormalization()(dense_input)\n",
    "    \n",
    "    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n",
    "    \n",
    "    feature_vector = concatenate([phrase_vector, dense_vector])\n",
    "    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n",
    "    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n",
    "    \n",
    "    output = Dense(3, activation=\"softmax\")(feature_vector)\n",
    "    \n",
    "    model = Model(inputs=[seq_input, dense_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index()\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 3\n",
    "train[\"fold_id\"] = train[\"index\"].apply(lambda x: x%NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(train[\"sentiment\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 3519 samples, validate on 1760 samples\n",
      "Epoch 1/15\n",
      "3519/3519 [==============================] - 7s 2ms/step - loss: 0.8448 - acc: 0.6689 - val_loss: 0.7516 - val_acc: 0.7369\n",
      "Epoch 2/15\n",
      "3519/3519 [==============================] - 5s 1ms/step - loss: 0.7683 - acc: 0.7184 - val_loss: 0.7518 - val_acc: 0.7369\n",
      "Epoch 3/15\n",
      "3519/3519 [==============================] - 5s 1ms/step - loss: 0.7468 - acc: 0.7181 - val_loss: 0.7478 - val_acc: 0.7369\n",
      "Epoch 00003: early stopping\n",
      "Predicting...\n",
      "2924/2924 [==============================] - 1s 379us/step\n",
      "\n",
      "FOLD 2\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 3519 samples, validate on 1760 samples\n",
      "Epoch 1/15\n",
      "3519/3519 [==============================] - 7s 2ms/step - loss: 0.8051 - acc: 0.7116 - val_loss: 0.7775 - val_acc: 0.7222\n",
      "Epoch 2/15\n",
      "3519/3519 [==============================] - 5s 1ms/step - loss: 0.7569 - acc: 0.7258 - val_loss: 0.7758 - val_acc: 0.7222\n",
      "Epoch 3/15\n",
      "3519/3519 [==============================] - 5s 1ms/step - loss: 0.7353 - acc: 0.7258 - val_loss: 0.7685 - val_acc: 0.7222\n",
      "Epoch 00003: early stopping\n",
      "Predicting...\n",
      "2924/2924 [==============================] - 1s 428us/step\n",
      "\n",
      "FOLD 3\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 3520 samples, validate on 1759 samples\n",
      "Epoch 1/15\n",
      "3520/3520 [==============================] - 8s 2ms/step - loss: 0.8276 - acc: 0.7037 - val_loss: 0.7773 - val_acc: 0.7146\n",
      "Epoch 2/15\n",
      "3520/3520 [==============================] - 5s 1ms/step - loss: 0.7482 - acc: 0.7295 - val_loss: 0.7878 - val_acc: 0.7146\n",
      "Epoch 3/15\n",
      "3520/3520 [==============================] - 5s 1ms/step - loss: 0.7251 - acc: 0.7307 - val_loss: 0.7769 - val_acc: 0.7123\n",
      "Epoch 00003: early stopping\n",
      "Predicting...\n",
      "2924/2924 [==============================] - 1s 462us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros((test.shape[0], 3))\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"FOLD\", i+1)\n",
    "    \n",
    "    print(\"Splitting the data into train and validation...\")\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "    model = build_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n",
    "              epochs=15, batch_size=128, shuffle=True, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    test_preds += model.predict([test_seq, test[dense_features]], batch_size=128, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07135761, 0.08300569, 0.84563673],\n",
       "       [0.13283553, 0.25722692, 0.60993755],\n",
       "       [0.04008221, 0.03586726, 0.92405053],\n",
       "       ...,\n",
       "       [0.17895204, 0.14232567, 0.6787223 ],\n",
       "       [0.08008266, 0.06767414, 0.85224319],\n",
       "       [0.10215181, 0.27115242, 0.62669577]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3825\n",
       "1     837\n",
       "0     617\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"sentiment\"].astype(int)\n",
    "test[[\"unique_hash\", \"sentiment\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "Use these predictions for the phrases which don't exist in train set...\n",
      "Make the submission ready...\n"
     ]
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"pred\"] = test_preds.argmax(axis=1)\n",
    "\n",
    "print(\"Use these predictions for the phrases which don't exist in train set...\")\n",
    "test.loc[test[\"sentiment\"].isnull(), \"sentiment\"] = test.loc[test[\"sentiment\"].isnull(), \"pred\"]\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "#test[\"sentiment\"] = test[\"sentiment\"].astype(int)\n",
    "#test[[\"unique_hash\", \"sentiment\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
